# AI Response Decision Architecture Analysis

## Current Approach: Inline Decision Making

### How It Works Now
Every incoming message goes through **multiple decision layers** in the `/api/messages/incoming` route:

```
Message arrives ‚Üí Check blocked ‚Üí Check manual mode ‚Üí Check staff activity ‚Üí 
Check context confidence ‚Üí Generate AI response
```

### Current Decision Layers

1. **Blocked Check** (Line 543-551)
   - Simple status check
   - ‚úÖ Fast, efficient

2. **Manual Mode Check** (Lines 556-698)
   - Queries staff messages
   - Checks time since last staff reply
   - Uses regex patterns (`shouldSwitchToAutoMode`)
   - ‚ö†Ô∏è **2 database queries per message**

3. **Staff Activity Check** (Lines 701-752)
   - Queries recent messages again
   - Checks 30-minute pause window
   - Uses `shouldAIRespond` with regex patterns
   - ‚ö†Ô∏è **Another database query**

4. **Context Confidence Check** (Lines 754-807)
   - Queries recent messages AGAIN
   - Uses regex first, AI if uncertain
   - ‚ö†Ô∏è **Another database query + potential AI call**

5. **AI Response Generation** (Lines 809-900+)
   - Finally generates response
   - ‚ö†Ô∏è **AI call (expensive)**

### Problems with Current Approach

‚ùå **Multiple Database Queries**
- 3-4 queries per message just for decision making
- Fetching same data multiple times
- Slow response time

‚ùå **Scattered Logic**
- Decision logic spread across 350+ lines
- Hard to maintain
- Duplicate checks

‚ùå **No Caching**
- Re-queries staff messages multiple times
- No conversation state caching

‚ùå **Reactive Only**
- Only runs when message arrives
- No proactive mode switching
- Conversations stuck in manual mode until customer messages

‚ùå **Expensive AI Calls**
- Context check uses AI even for simple decisions
- Name extraction uses AI
- No batching or optimization

---

## Alternative Approaches

### Option 1: AI-First Decision Making ‚≠ê RECOMMENDED

**Concept:** Let AI decide whether to respond BEFORE generating response

```typescript
// Single AI call to decide + respond
const decision = await ai.analyzeAndRespond({
  message: "When are you open?",
  conversationHistory: [...],
  conversationStatus: "manual",
  lastStaffReply: "2 hours ago"
})

// AI returns:
{
  shouldRespond: true,
  reason: "Simple hours query - I can answer this",
  suggestedMode: "auto",
  response: "We're open 9-5 Monday to Friday!"
}
```

**Pros:**
- ‚úÖ Single AI call (cheaper than multiple checks + generation)
- ‚úÖ More intelligent decisions
- ‚úÖ Handles edge cases better
- ‚úÖ Can explain reasoning
- ‚úÖ Adapts to context naturally

**Cons:**
- ‚ö†Ô∏è Slightly higher latency (but only 1 call vs 2-3)
- ‚ö†Ô∏è Requires good prompt engineering

**Implementation:**
```typescript
// New file: app/lib/ai/decision-engine.ts
export async function analyzeAndDecide(params) {
  const prompt = `
You are an AI assistant decision engine. Analyze this message and decide:
1. Should AI respond or wait for staff?
2. Should conversation mode change?
3. What response (if any)?

Conversation Status: ${params.status}
Last Staff Reply: ${params.lastStaffReply}
Customer Message: "${params.message}"

Recent Context:
${params.history.map(m => `${m.sender}: ${m.text}`).join('\n')}

Return JSON:
{
  "shouldRespond": boolean,
  "reason": string,
  "suggestedMode": "auto" | "manual" | "blocked",
  "response": string | null
}
`

  const result = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    response_format: { type: "json_object" },
    messages: [{ role: "user", content: prompt }]
  })
  
  return JSON.parse(result.choices[0].message.content)
}
```

---

### Option 2: Database Functions + Triggers

**Concept:** Move logic to PostgreSQL functions

```sql
-- Function to check if conversation should auto-switch
CREATE OR REPLACE FUNCTION should_auto_switch_mode(
  conversation_id UUID,
  new_message TEXT
) RETURNS BOOLEAN AS $$
DECLARE
  last_staff_time TIMESTAMP;
  minutes_since_staff INTEGER;
BEGIN
  -- Get last staff message time
  SELECT created_at INTO last_staff_time
  FROM messages
  WHERE conversation_id = $1 AND sender = 'staff'
  ORDER BY created_at DESC
  LIMIT 1;
  
  -- If no staff message, switch to auto
  IF last_staff_time IS NULL THEN
    RETURN TRUE;
  END IF;
  
  -- Calculate minutes since staff reply
  minutes_since_staff := EXTRACT(EPOCH FROM (NOW() - last_staff_time)) / 60;
  
  -- Auto-switch if 30+ minutes
  IF minutes_since_staff > 30 THEN
    RETURN TRUE;
  END IF;
  
  -- Check message patterns (simple queries)
  IF new_message ~* 'when.*open|what.*hours|how much|where.*located' THEN
    RETURN TRUE;
  END IF;
  
  RETURN FALSE;
END;
$$ LANGUAGE plpgsql;

-- Trigger to auto-update conversation status
CREATE TRIGGER auto_switch_conversation_mode
AFTER INSERT ON messages
FOR EACH ROW
WHEN (NEW.sender = 'customer')
EXECUTE FUNCTION update_conversation_mode();
```

**Pros:**
- ‚úÖ Very fast (runs in database)
- ‚úÖ No API calls needed
- ‚úÖ Automatic execution
- ‚úÖ Reduces application code

**Cons:**
- ‚ö†Ô∏è Limited pattern matching (regex only)
- ‚ö†Ô∏è Harder to debug
- ‚ö†Ô∏è Less flexible than AI

---

### Option 3: Cron Job + Background Processing

**Concept:** Periodic job to reset stale manual conversations

```typescript
// Vercel Cron: /api/cron/reset-stale-conversations
export async function GET() {
  const { data: staleConversations } = await supabase
    .from('conversations')
    .select('id, updated_at')
    .eq('status', 'manual')
    .lt('updated_at', new Date(Date.now() - 30 * 60 * 1000).toISOString())
  
  // Reset to auto mode
  await supabase
    .from('conversations')
    .update({ status: 'auto' })
    .in('id', staleConversations.map(c => c.id))
  
  return Response.json({ reset: staleConversations.length })
}
```

**Pros:**
- ‚úÖ Proactive (doesn't wait for customer message)
- ‚úÖ Simple logic
- ‚úÖ Runs independently

**Cons:**
- ‚ö†Ô∏è Not real-time (runs every X minutes)
- ‚ö†Ô∏è Doesn't handle message-specific decisions
- ‚ö†Ô∏è Good as supplement, not replacement

---

### Option 4: Hybrid Approach ‚≠ê‚≠ê BEST

**Concept:** Combine AI decision-making with database optimization

```typescript
// 1. Cache conversation state in memory/Redis
const conversationState = await getConversationState(conversationId)

// 2. Quick regex checks first
if (isBlocked(conversationState)) return
if (isSimpleQuery(message)) {
  // Fast path - no AI needed
  return await generateSimpleResponse(message)
}

// 3. AI decision for complex cases
const decision = await ai.analyzeAndDecide({
  message,
  state: conversationState,
  history: await getCachedHistory(conversationId)
})

// 4. Background cron to reset stale conversations
// Runs every 5 minutes
```

**Pros:**
- ‚úÖ Fast for simple cases (regex)
- ‚úÖ Intelligent for complex cases (AI)
- ‚úÖ Proactive cleanup (cron)
- ‚úÖ Cached data (fewer queries)

**Cons:**
- ‚ö†Ô∏è More complex architecture
- ‚ö†Ô∏è Requires caching layer

---

## Recommended Solution

### **Hybrid Approach with AI-First for Complex Decisions**

**Phase 1: Immediate Improvements (No Architecture Change)**

1. **Consolidate Database Queries**
   ```typescript
   // Single query for all decision data
   const decisionData = await supabase
     .from('conversations')
     .select(`
       *,
       messages(sender, text, created_at)
     `)
     .eq('id', conversationId)
     .single()
   
   // Use this data for ALL checks
   ```

2. **Add Cron Job for Stale Conversations**
   ```typescript
   // vercel.json
   {
     "crons": [{
       "path": "/api/cron/reset-stale-conversations",
       "schedule": "*/5 * * * *"  // Every 5 minutes
     }]
   }
   ```

3. **Cache Conversation State**
   ```typescript
   // Use Vercel KV or simple in-memory cache
   const state = await cache.get(`conv:${id}`)
   ```

**Phase 2: AI-First Decision Engine (Better Long-term)**

1. **Create Decision Engine**
   - Single AI call analyzes message + context
   - Returns: shouldRespond, reason, mode, response
   - Replaces all regex checks

2. **Keep Simple Query Fast Path**
   - Regex for "when are you open?" ‚Üí instant response
   - Complex questions ‚Üí AI decision engine

3. **Background Jobs**
   - Reset stale conversations (every 5 min)
   - Cleanup old alerts (daily)
   - Analytics (hourly)

---

## Cost Comparison

### Current Approach
```
Per message:
- 3-4 database queries: ~40ms
- Context check (sometimes): $0.0001
- Response generation: $0.001
Total: ~50ms, $0.0011 per message
```

### AI-First Approach
```
Per message:
- 1 database query: ~10ms
- AI decision + response: $0.0015
Total: ~20ms, $0.0015 per message
```

**Verdict:** AI-First is **faster** (20ms vs 50ms) and only **36% more expensive** ($0.0015 vs $0.0011)

---

## Implementation Priority

### üî• Do Now (Quick Wins)
1. ‚úÖ Consolidate database queries (done in this session)
2. ‚úÖ Add cron job for stale conversations
3. ‚úÖ Fix manual mode switching logic (done)

### üìÖ Do Next Week (Medium Effort)
4. Add conversation state caching
5. Optimize AI calls (batch where possible)
6. Add simple query fast path

### üéØ Do Next Month (Architecture Change)
7. Build AI-first decision engine
8. Migrate to hybrid approach
9. Add comprehensive monitoring

---

## Conclusion

**Current system works but is inefficient:**
- Too many database queries
- Scattered logic
- No caching
- Reactive only

**Recommended path forward:**
1. **Immediate:** Add cron job + consolidate queries ‚úÖ
2. **Short-term:** Add caching + fast path for simple queries
3. **Long-term:** Move to AI-first decision engine

This gives you **better performance, lower costs, and smarter decisions** without a massive rewrite.
